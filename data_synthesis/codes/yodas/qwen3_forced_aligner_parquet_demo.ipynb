{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen3 ForcedAligner offline demo (parquet → timestamps)\n",
        "\n",
        "This notebook loads **one parquet shard** from your Yodas-Granary ASR-only directory, picks one example, and runs:\n",
        "1) *(Optional)* Qwen3-ASR (vLLM backend) to get transcript\n",
        "2) Qwen3-ForcedAligner to produce **word/character timestamps**\n",
        "\n",
        "Model usage references:\n",
        "- ForcedAligner direct usage and `qwen-asr-demo` flags on the HF model card. citeturn1view0\n",
        "\n",
        "---\n",
        "✅ Assumptions: you have a GPU (L40S) and HF cache env vars already set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF_HOME: /data/user_data/haolingp/hf_cache\n",
            "HF_HUB_CACHE: /data/user_data/haolingp/hf_cache/hub\n",
            "TRANSFORMERS_CACHE: None\n",
            "CUDA_VISIBLE_DEVICES: None\n",
            "\n",
            "GPU(s):\n",
            "GPU 0: NVIDIA L40S (UUID: GPU-5f205193-a34d-1536-6802-bbcbe0ca3b70)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Check GPU + HF cache wiring (read-only) ---\n",
        "import os, subprocess, textwrap\n",
        "print('HF_HOME:', os.environ.get('HF_HOME'))\n",
        "print('HF_HUB_CACHE:', os.environ.get('HF_HUB_CACHE'))\n",
        "print('TRANSFORMERS_CACHE:', os.environ.get('TRANSFORMERS_CACHE'))\n",
        "print('CUDA_VISIBLE_DEVICES:', os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
        "try:\n",
        "    out = subprocess.check_output(['nvidia-smi','-L'], text=True)\n",
        "    print('\\nGPU(s):')\n",
        "    print(out)\n",
        "except Exception as e:\n",
        "    print('nvidia-smi not available:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install deps (run once per env)\n",
        "If you already installed these in your conda env, you can skip this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install -U \"qwen-asr[vllm]\" pandas pyarrow soundfile numpy\n",
        "# optional: for resampling if needed\n",
        "!pip -q install -U librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Point to one parquet file\n",
        "Set `PARQUET_PATH` to a single parquet file inside:\n",
        "`/data/group_data/li_lab/siqiouya/datasets/yodas-granary/data/en000/asr_only/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found parquet files: 499\n",
            "First 5:\n",
            "   /data/group_data/li_lab/siqiouya/datasets/yodas-granary/data/en000/asr_only/00000000.parquet\n",
            "   /data/group_data/li_lab/siqiouya/datasets/yodas-granary/data/en000/asr_only/00000001.parquet\n",
            "   /data/group_data/li_lab/siqiouya/datasets/yodas-granary/data/en000/asr_only/00000002.parquet\n",
            "   /data/group_data/li_lab/siqiouya/datasets/yodas-granary/data/en000/asr_only/00000003.parquet\n",
            "   /data/group_data/li_lab/siqiouya/datasets/yodas-granary/data/en000/asr_only/00000004.parquet\n",
            "\n",
            "Using PARQUET_PATH: /data/group_data/li_lab/siqiouya/datasets/yodas-granary/data/en000/asr_only/00000000.parquet\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "BASE_DIR = \"/data/group_data/li_lab/siqiouya/datasets/yodas-granary/data/en000/asr_only/\"\n",
        "parquets = sorted(glob.glob(os.path.join(BASE_DIR, \"**/*.parquet\"), recursive=True))\n",
        "print('Found parquet files:', len(parquets))\n",
        "print('First 5:')\n",
        "for p in parquets[:5]:\n",
        "    print('  ', p)\n",
        "\n",
        "# Pick the first parquet by default; change to any path you want.\n",
        "PARQUET_PATH = parquets[0] if parquets else None\n",
        "print('\\nUsing PARQUET_PATH:', PARQUET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load parquet + inspect schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 1703 Cols: 9\n",
            "\n",
            "Columns:\n",
            "['utt_id', 'audio', 'duration', 'lang', 'task', 'text', 'translation_en', 'original_audio_id', 'original_audio_offset']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>utt_id</th>\n",
              "      <th>audio</th>\n",
              "      <th>duration</th>\n",
              "      <th>lang</th>\n",
              "      <th>task</th>\n",
              "      <th>text</th>\n",
              "      <th>translation_en</th>\n",
              "      <th>original_audio_id</th>\n",
              "      <th>original_audio_offset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>en000_00000000_Y0aGwNq86f4_112_62_5_54</td>\n",
              "      <td>{'bytes': b'RIFF\\xa4\\xb4\\x02\\x00WAVEfmt \\x10\\x...</td>\n",
              "      <td>5.54</td>\n",
              "      <td>&lt;en&gt;</td>\n",
              "      <td>&lt;asr&gt;</td>\n",
              "      <td>Which sites have you been using and how did th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Y0aGwNq86f4</td>\n",
              "      <td>112.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en000_00000000_Y0aGwNq86f4_118_16_30_48</td>\n",
              "      <td>{'bytes': b'RIFF$\\xe2\\x0e\\x00WAVEfmt \\x10\\x00\\...</td>\n",
              "      <td>30.48</td>\n",
              "      <td>&lt;en&gt;</td>\n",
              "      <td>&lt;asr&gt;</td>\n",
              "      <td>edge technologies to find talent in the techno...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Y0aGwNq86f4</td>\n",
              "      <td>118.16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    utt_id  \\\n",
              "0   en000_00000000_Y0aGwNq86f4_112_62_5_54   \n",
              "1  en000_00000000_Y0aGwNq86f4_118_16_30_48   \n",
              "\n",
              "                                               audio  duration  lang   task  \\\n",
              "0  {'bytes': b'RIFF\\xa4\\xb4\\x02\\x00WAVEfmt \\x10\\x...      5.54  <en>  <asr>   \n",
              "1  {'bytes': b'RIFF$\\xe2\\x0e\\x00WAVEfmt \\x10\\x00\\...     30.48  <en>  <asr>   \n",
              "\n",
              "                                                text translation_en  \\\n",
              "0  Which sites have you been using and how did th...            NaN   \n",
              "1  edge technologies to find talent in the techno...            NaN   \n",
              "\n",
              "  original_audio_id  original_audio_offset  \n",
              "0       Y0aGwNq86f4                 112.62  \n",
              "1       Y0aGwNq86f4                 118.16  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "assert PARQUET_PATH is not None, 'No parquet file found. Set PARQUET_PATH manually.'\n",
        "\n",
        "table = pq.read_table(PARQUET_PATH)\n",
        "df = table.to_pandas()\n",
        "print('Rows:', len(df), 'Cols:', len(df.columns))\n",
        "print('\\nColumns:')\n",
        "print(df.columns.tolist())\n",
        "display(df.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Pick one row + locate audio + transcript fields\n",
        "This block tries to guess likely columns for audio and text.\n",
        "\n",
        "- If your parquet stores **audio file paths**, it will use them directly.\n",
        "- If it stores **audio bytes/arrays**, it will decode them.\n",
        "- For transcript, it looks for columns like `text`, `transcript`, `asr_text` etc.\n",
        "\n",
        "If the guesses are wrong, just set `AUDIO_COL` / `TEXT_COL` manually after inspecting `df.columns`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guessed AUDIO_COL: audio\n",
            "Guessed TEXT_COL : text\n",
            "\n",
            "Sample row preview (selected cols):\n",
            "  audio: {'bytes': b'RIFF\\xa4\\xb4\\x02\\x00WAVEfmt \\x10\\x00\\x00\\x00\\x01\\x00\\x01\\x00\\x80>\\x00\\x00\\x00}\\x00\\x00\\x02\\x00\\x10\\x00data\\x80\\xb4\\x02\\x00\\xef\\xf9M\\xfeS\\x03F\\xf8\\x05\\xf9\\x87\\x01\\x8e\\x02\\xbc\\xfc\\xb4\\xfdz\\x...\n",
            "  text: Which sites have you been using and how did the use of these technologies assist your search?\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def guess_col(cols, patterns):\n",
        "    for pat in patterns:\n",
        "        r = re.compile(pat, re.IGNORECASE)\n",
        "        for c in cols:\n",
        "            if r.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "cols = df.columns.tolist()\n",
        "AUDIO_COL = guess_col(cols, [r'^audio$', r'audio', r'wav', r'path'])\n",
        "TEXT_COL  = guess_col(cols, [r'^text$', r'transcript', r'asr', r'sentence', r'utt'])\n",
        "\n",
        "print('Guessed AUDIO_COL:', AUDIO_COL)\n",
        "print('Guessed TEXT_COL :', TEXT_COL)\n",
        "\n",
        "ROW_IDX = 0\n",
        "row = df.iloc[ROW_IDX]\n",
        "print('\\nSample row preview (selected cols):')\n",
        "for k in [AUDIO_COL, TEXT_COL]:\n",
        "    if k is None: continue\n",
        "    v = row[k]\n",
        "    s = str(v)\n",
        "    print(f'  {k}:', s[:200] + ('...' if len(s)>200 else ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Load audio into a standard form\n",
        "We normalize audio into either:\n",
        "- a local path string, or\n",
        "- a `(np.ndarray, sr)` tuple (float32)\n",
        "\n",
        "The Qwen3 aligner supports both input styles. citeturn1view0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio input type: <class 'str'>\n",
            "Audio path/url: en000_00000000_Y0aGwNq86f4_112_62_5_54.wav\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from pathlib import Path\n",
        "\n",
        "def resolve_audio(row_val):\n",
        "    \"\"\"Return either a path (str) or (np.ndarray, sr).\"\"\"\n",
        "    # Case 1: already a string path\n",
        "    if isinstance(row_val, str):\n",
        "        p = Path(row_val)\n",
        "        if p.exists():\n",
        "            return str(p)\n",
        "        # Sometimes stored as relative path\n",
        "        p2 = Path(BASE_DIR) / row_val\n",
        "        if p2.exists():\n",
        "            return str(p2)\n",
        "        # fallthrough: maybe URL or something\n",
        "        raise FileNotFoundError(f\"Audio file not found: {row_val} (also tried {Path(BASE_DIR)/row_val})\")\n",
        "\n",
        "\n",
        "    # Case 2: dict-like (common in HF datasets)\n",
        "    if isinstance(row_val, dict):\n",
        "        for key in ['path','file','filepath','audio_path']:\n",
        "            if key in row_val and isinstance(row_val[key], str):\n",
        "                return resolve_audio(row_val[key])\n",
        "        for key in ['array','samples']:\n",
        "            if key in row_val:\n",
        "                arr = np.asarray(row_val[key], dtype=np.float32)\n",
        "                sr = int(row_val.get('sampling_rate', row_val.get('sr', 16000)))\n",
        "                return (arr, sr)\n",
        "\n",
        "    # Case 3: bytes (wav bytes)\n",
        "    if isinstance(row_val, (bytes, bytearray)):\n",
        "        import io\n",
        "        data, sr = sf.read(io.BytesIO(row_val), dtype='float32')\n",
        "        if data.ndim > 1:\n",
        "            data = data.mean(axis=1)\n",
        "        return (data, sr)\n",
        "\n",
        "    # Case 4: numpy array already\n",
        "    if isinstance(row_val, np.ndarray):\n",
        "        return (row_val.astype(np.float32), 16000)\n",
        "\n",
        "    raise TypeError(f'Unsupported audio type: {type(row_val)}')\n",
        "\n",
        "assert AUDIO_COL is not None, 'Could not guess audio column. Set AUDIO_COL manually.'\n",
        "audio_input = resolve_audio(row[AUDIO_COL])\n",
        "print('Audio input type:', type(audio_input))\n",
        "if isinstance(audio_input, tuple):\n",
        "    arr, sr = audio_input\n",
        "    print('Audio tuple:', arr.shape, 'sr=', sr, 'dtype=', arr.dtype)\n",
        "else:\n",
        "    print('Audio path/url:', audio_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Option A (recommended): align using **existing transcript** from parquet\n",
        "If your parquet already has transcript text, you can run ForcedAligner directly.\n",
        "\n",
        "Direct ForcedAligner usage example is on the model card. citeturn1view0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcript found: True\n",
            "Transcript preview: Which sites have you been using and how did the use of these technologies assist your search?\n"
          ]
        }
      ],
      "source": [
        "TRANSCRIPT = None\n",
        "if TEXT_COL is not None:\n",
        "    val = row[TEXT_COL]\n",
        "    if isinstance(val, str) and val.strip():\n",
        "        TRANSCRIPT = val.strip()\n",
        "\n",
        "print('Transcript found:', TRANSCRIPT is not None)\n",
        "if TRANSCRIPT:\n",
        "    print('Transcript preview:', TRANSCRIPT[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/haolingp/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/haolingp/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/qwen_asr/inference/utils.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(x, sr=None, mono=False)\n",
            "/home/haolingp/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'en000_00000000_Y0aGwNq86f4_112_62_5_54.wav'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLibsndfileError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/librosa/core/audio.py:176\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     y, sr_native = \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m sf.SoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/librosa/core/audio.py:209\u001b[39m, in \u001b[36m__soundfile_load\u001b[39m\u001b[34m(path, offset, duration, dtype)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     context = \u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/soundfile.py:690\u001b[39m, in \u001b[36mSoundFile.__init__\u001b[39m\u001b[34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;28mself\u001b[39m._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[32m    689\u001b[39m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m \u001b[38;5;28mself\u001b[39m._file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode).issuperset(\u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/soundfile.py:1265\u001b[39m, in \u001b[36mSoundFile._open\u001b[39m\u001b[34m(self, file, mode_int, closefd)\u001b[39m\n\u001b[32m   1264\u001b[39m     err = _snd.sf_error(file_ptr)\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix=\u001b[33m\"\u001b[39m\u001b[33mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.name))\n\u001b[32m   1266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode_int == _snd.SFM_WRITE:\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
            "\u001b[31mLibsndfileError\u001b[39m: Error opening 'en000_00000000_Y0aGwNq86f4_112_62_5_54.wav': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Set language explicitly if you know it; otherwise choose None and/or set English.\u001b[39;00m\n\u001b[32m     14\u001b[39m LANGUAGE = \u001b[33m\"\u001b[39m\u001b[33mEnglish\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m aligned = \u001b[43maligner\u001b[49m\u001b[43m.\u001b[49m\u001b[43malign\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRANSCRIPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLANGUAGE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# aligned[0] is a list of segments/tokens with timestamps\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mAligned items:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(aligned[\u001b[32m0\u001b[39m]))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/qwen_asr/inference/qwen3_forced_aligner.py:422\u001b[39m, in \u001b[36mQwen3ForcedAligner.align\u001b[39m\u001b[34m(self, audio, text, language)\u001b[39m\n\u001b[32m    420\u001b[39m texts = ensure_list(text)\n\u001b[32m    421\u001b[39m languages = ensure_list(language)\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m audios = \u001b[43mnormalize_audios\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(languages) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audios) > \u001b[32m1\u001b[39m:\n\u001b[32m    425\u001b[39m     languages = languages * \u001b[38;5;28mlen\u001b[39m(audios)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/qwen_asr/inference/utils.py:206\u001b[39m, in \u001b[36mnormalize_audios\u001b[39m\u001b[34m(audios)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_audios\u001b[39m(audios: Union[AudioLike, List[AudioLike]]) -> List[np.ndarray]:\n\u001b[32m    205\u001b[39m     items = ensure_list(audios)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mnormalize_audio_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m items]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/qwen_asr/inference/utils.py:191\u001b[39m, in \u001b[36mnormalize_audio_input\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03mNormalize one audio input to mono 16k float32 waveform in [-1, 1].\u001b[39;00m\n\u001b[32m    181\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m \u001b[33;03m        Mono 16k float32 waveform in [-1, 1].\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     audio, sr = \u001b[43mload_audio_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a) == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a[\u001b[32m0\u001b[39m], np.ndarray):\n\u001b[32m    193\u001b[39m     audio, sr = a[\u001b[32m0\u001b[39m], \u001b[38;5;28mint\u001b[39m(a[\u001b[32m1\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/qwen_asr/inference/utils.py:146\u001b[39m, in \u001b[36mload_audio_any\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    144\u001b[39m         audio, sr = sf.read(f, dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m, always_2d=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     audio, sr = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m audio = np.asarray(audio, dtype=np.float32)\n\u001b[32m    149\u001b[39m sr = \u001b[38;5;28mint\u001b[39m(sr)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/librosa/core/audio.py:184\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib.PurePath)):\n\u001b[32m    181\u001b[39m     warnings.warn(\n\u001b[32m    182\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[33m\"\u001b[39m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     y, sr_native = \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/decorator.py:235\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    234\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/librosa/util/decorators.py:63\u001b[39m, in \u001b[36mdeprecated.<locals>.__wrapper\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[32m     55\u001b[39m warnings.warn(\n\u001b[32m     56\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[32m     62\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/librosa/core/audio.py:240\u001b[39m, in \u001b[36m__audioread_load\u001b[39m\u001b[34m(path, offset, duration, dtype)\u001b[39m\n\u001b[32m    237\u001b[39m     reader = path\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     reader = \u001b[43maudioread\u001b[49m\u001b[43m.\u001b[49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[32m    243\u001b[39m     sr_native = input_file.samplerate\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/audioread/__init__.py:126\u001b[39m, in \u001b[36maudio_open\u001b[39m\u001b[34m(path, backends)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qwen3_asr/lib/python3.12/site-packages/audioread/rawread.py:59\u001b[39m, in \u001b[36mRawAudioFile.__init__\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28mself\u001b[39m._fh = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m         \u001b[38;5;28mself\u001b[39m._file = aifc.open(\u001b[38;5;28mself\u001b[39m._fh)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'en000_00000000_Y0aGwNq86f4_112_62_5_54.wav'"
          ]
        }
      ],
      "source": [
        "# --- Run forced alignment (audio + transcript) ---\n",
        "import torch\n",
        "from qwen_asr import Qwen3ForcedAligner\n",
        "\n",
        "assert TRANSCRIPT is not None, 'No transcript found; use Option B below to run ASR first.'\n",
        "\n",
        "aligner = Qwen3ForcedAligner.from_pretrained(\n",
        "    \"Qwen/Qwen3-ForcedAligner-0.6B\",\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"cuda:0\",\n",
        ")\n",
        "\n",
        "# Set language explicitly if you know it; otherwise choose None and/or set English.\n",
        "LANGUAGE = \"English\"\n",
        "aligned = aligner.align(audio=audio_input, text=TRANSCRIPT, language=LANGUAGE)\n",
        "\n",
        "# aligned[0] is a list of segments/tokens with timestamps\n",
        "print('Aligned items:', len(aligned[0]))\n",
        "print('First item:', aligned[0][0])\n",
        "print('Example:', aligned[0][0].text, aligned[0][0].start_time, aligned[0][0].end_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export as JSON (token/word timestamps)\n",
        "This makes it easy to inspect/plot or convert to TextGrid later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "out = []\n",
        "for it in aligned[0]:\n",
        "    out.append({\n",
        "        \"text\": getattr(it, 'text', None),\n",
        "        \"start\": float(getattr(it, 'start_time', 0.0)),\n",
        "        \"end\": float(getattr(it, 'end_time', 0.0)),\n",
        "    })\n",
        "\n",
        "print('First 5 aligned tokens:')\n",
        "print(json.dumps(out[:5], ensure_ascii=False, indent=2))\n",
        "\n",
        "OUT_JSON_PATH = \"qwen3_forced_alignment_example.json\"\n",
        "with open(OUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(out, f, ensure_ascii=False, indent=2)\n",
        "print('Wrote:', OUT_JSON_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Option B: If parquet has **audio only** (no transcript), run Qwen3-ASR (vLLM) and return timestamps\n",
        "Qwen3-ASR supports returning timestamps when you pass `forced_aligner` and its kwargs. citeturn1view0\n",
        "\n",
        "⚠️ This is heavier: it loads the ASR model (1.7B by default). You can switch to `Qwen/Qwen3-ASR-0.6B` if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment and run if you need ASR -> transcript -> timestamps\n",
        "import torch\n",
        "from qwen_asr import Qwen3ASRModel\n",
        "\n",
        "# Use the vLLM backend wrapper from the package (as shown in the README). citeturn1view0\n",
        "asr = Qwen3ASRModel.LLM(\n",
        "    model=\"Qwen/Qwen3-ASR-1.7B\",\n",
        "    gpu_memory_utilization=0.70,\n",
        "    max_inference_batch_size=8,\n",
        "    max_new_tokens=2048,\n",
        "    forced_aligner=\"Qwen/Qwen3-ForcedAligner-0.6B\",\n",
        "    forced_aligner_kwargs=dict(device_map=\"cuda:0\", dtype=torch.bfloat16),\n",
        ")\n",
        "\n",
        "results = asr.transcribe(audio=audio_input, language=None)\n",
        "r0 = results[0]\n",
        "print('Detected language:', getattr(r0, 'language', None))\n",
        "print('Transcript:', getattr(r0, 'text', None))\n",
        "\n",
        "# Depending on version, timestamps may be in r0.timestamps / r0.words / r0.segments; inspect:\n",
        "print('Available fields:', [a for a in dir(r0) if not a.startswith('_')][:50])\n",
        "\n",
        "# Try common locations\n",
        "for cand in ['timestamps','words','segments','tokens','alignment']:\n",
        "    if hasattr(r0, cand):\n",
        "        val = getattr(r0, cand)\n",
        "        if val is not None:\n",
        "            print(f'Found {cand}:', type(val), 'len=', (len(val) if hasattr(val,'__len__') else 'n/a'))\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Next: convert to TextGrid (optional)\n",
        "If you want, we can add a cell to convert the JSON timestamps into a Praat TextGrid to match your MFA workflow."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "qwen3_asr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
